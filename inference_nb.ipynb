{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import boto3\n",
    "from torchvision import transforms\n",
    "#import deepspeed\n",
    "import json\n",
    "from yarn_test import Yarn\n",
    "from io import BytesIO\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the configuration file\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_dict = json.load(config_file)\n",
    "\n",
    "\n",
    "from modeling_cogvlm import CogVLMForCausalLM, CogVLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# # Define the S3 bucket and object key\n",
    "# bucket_name = 'mock-cogvlm-test'\n",
    "# object_key = 'model/mino_chat_exp5.bin'\n",
    "\n",
    "# # Create a S3 client\n",
    "# s3_client = boto3.client('s3')\n",
    "\n",
    "# # Specify the path to save the model on your SageMaker instance\n",
    "# local_model_path = '/home/ec2-user/SageMaker/testing/mino_chat_exp5.bin'\n",
    "\n",
    "# # Download the model from S3 to your notebook environment\n",
    "# s3_client.download_file(bucket_name, object_key, local_model_path)\n",
    "\n",
    "# # If the model is a PyTorch model, you might load it like this:\n",
    "# # (Make sure you have the correct environment and dependencies installed)\n",
    "\n",
    "# # import torch\n",
    "# # model = torch.load(local_model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# # If it's a TensorFlow model, it might look like this:\n",
    "# # from tensorflow.keras.models import load_model\n",
    "# # model = load_model(local_model_path)\n",
    "\n",
    "# # Now you can use the model for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5474df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE_TOKEN_TYPE = 0\n",
    "VISION_TOKEN_TYPE = 1\n",
    "\n",
    "\n",
    "def calculate_num_image_tokens(image_size, patch_size):\n",
    "    num_patches_per_side = image_size // patch_size\n",
    "    return (num_patches_per_side ** 2) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89008284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(model, tokenizer, transform, query, image_bytes):\n",
    "    \n",
    "    \n",
    "    \n",
    "#     image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "    \n",
    "    #image = [transform(image)]\n",
    "    \n",
    "    #images_processed = []\n",
    "    \n",
    "#     for img_list in image:\n",
    "#         processed_imgs = [img.to('cuda').half() for img in img_list]\n",
    "#         images_processed.append(processed_imgs)\n",
    "    \n",
    "    #image = image.to('cuda').half()\n",
    "    \n",
    "#     input_ids = [tokenizer.bos_token_id]\n",
    "#     token_type_ids = [LANGUAGE_TOKEN_TYPE]\n",
    "\n",
    "#     vision_token_num = calculate_num_image_tokens(image_size=224, patch_size=14)\n",
    "\n",
    "#     # Add vision tokens placeholder for each image\n",
    "#     input_ids += [tokenizer.pad_token_id] * vision_token_num\n",
    "#     token_type_ids += [VISION_TOKEN_TYPE] * (vision_token_num)\n",
    "\n",
    "#     # Tokenize the question and append\n",
    "#     query_ids = tokenizer(query, truncation=True, max_length=2048, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze(0)\n",
    "#     input_ids += query_ids.tolist()\n",
    "#     token_type_ids += [LANGUAGE_TOKEN_TYPE] * len(query_ids)\n",
    "    \n",
    "#     attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "#     attention_mask  = torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0).to('cuda')\n",
    "    \n",
    "#     input_ids = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to('cuda')\n",
    "#     token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0).to('cuda')\n",
    "    \n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(input_ids=input_ids, token_type_ids=token_type_ids, images=images_processed, attention_mask=attention_mask)\n",
    "        \n",
    "       \n",
    "#         generated_ids = outputs[0]  \n",
    "\n",
    "#         # Decode the token ids to text\n",
    "#         generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "#         print(\"Generated text:\", generated_text)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_position_embedding_size(model, new_num_positions=257):\n",
    "    vision_model = model.vision  # Adjust this path based on your model's structure\n",
    "    for layer in vision_model.children():\n",
    "        if isinstance(layer, PatchEmbedding):  \n",
    "            new_position_embedding = torch.nn.Embedding(new_num_positions, layer.position_embedding.embedding_dim)\n",
    "            # Copy existing weights for positions that overlap\n",
    "            if new_num_positions <= layer.position_embedding.weight.size(0):\n",
    "                new_position_embedding.weight.data[:new_num_positions] = layer.position_embedding.weight.data[:new_num_positions]\n",
    "            else:\n",
    "                raise ValueError(\"New number of positions exceeds the size of the existing weights\")\n",
    "            layer.position_embedding = new_position_embedding\n",
    "            break  # Assuming there's only one PatchEmbedding layer to modify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rotary_embeddings(model):\n",
    "    # Iterate over all named modules in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the current module has a 'rotary_emb' attribute\n",
    "        if hasattr(module, 'rotary_emb'):\n",
    "            # Calculate the new dimension based on model configuration\n",
    "            dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "            \n",
    "            # Instantiate your replacement embedding\n",
    "            yarn_rope = Yarn(dim)\n",
    "            \n",
    "            # Replace the rotary_emb with yarn_rope\n",
    "            setattr(module, 'rotary_emb', yarn_rope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559d9cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5') \n",
    "\n",
    "\n",
    "config_path = 'config.json'\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config_dict = json.load(config_file)\n",
    "\n",
    "# Create the configuration object for your model\n",
    "config = CogVLMConfig(**config_dict)\n",
    "\n",
    "# Initialize your model with the configuration\n",
    "model = CogVLMForCausalLM(config)\n",
    "#print(model)\n",
    "\n",
    "\n",
    "# replace_rotary_embeddings(model)\n",
    "# adjust_position_embedding_size(model, new_num_positions=257)\n",
    "# Load the model weights\n",
    "model_state_dict = torch.load(\"mino_chat_exp9.bin\")\n",
    "\n",
    "#state_dict = {k.replace(\"model.\", \"\"): v for k, v in model_state_dict.items()}\n",
    "#print(f'state_dict:{model_state_dict.keys()}')\n",
    "model.load_state_dict(model_state_dict)\n",
    "#print(f'model after:{model}')\n",
    "\n",
    "# new_num_positions = 257\n",
    "# config.vision_config['num_positions'] = new_num_positions\n",
    "\n",
    "# #Directly modify the position_embedding in the PatchEmbedding layer\n",
    "# vision_model = model.vision\n",
    "# for layer in vision_model.children():\n",
    "#     if layer.__class__.__name__ == 'PatchEmbedding':\n",
    "#         new_position_embedding = torch.nn.Embedding(new_num_positions, layer.position_embedding.embedding_dim)\n",
    "#         new_position_embedding.weight.data[:257] = layer.position_embedding.weight.data[:257]\n",
    "#         layer.position_embedding = new_position_embedding\n",
    "\n",
    "\n",
    "# Move model to GPU and set it to evaluation mode\n",
    "#adjust_position_embedding_size(model, new_num_positions=257)\n",
    "model.to('cuda').bfloat16().eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference(model=model, tokenizer=tokenizer, transform=transform, query=query, image_bytes=image_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90574560",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('41_jpg.rf.7c258d236d88a4304878a19405b61b5c.jpg', 'rb') as f:\n",
    "    image_bytes = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(BytesIO(image_bytes)).convert('RGB')\n",
    "# assistant_text1 = \"\"\"This image depicts a Low squamous intra-epithelial lesion, an indicator of initial changes in cervical cells that are usually not a cause for immediate concern. \"\"\"\n",
    "# user_text1 = \"\"\"Give further reasons\"\"\"\n",
    "\n",
    "# assistant_text2 = \"\"\"Low squamous intra-epithelial lesion is usually not a cause for immediate concern, but it can still be a precursor to cancer.\"\"\" \n",
    "\n",
    "# user_text2 = \"\"\"what are the causes of low squamous intra-epithelial?\"\"\"\n",
    " \n",
    "inputs = model.build_conversation_input_ids(tokenizer, history=[], query=\"USER: what medical image is this? ASSISTANT:\", \n",
    "                                            images=[image], \n",
    "                                            template_version='chat')  \n",
    "\n",
    "# inputs = model.build_conversation_input_ids(tokenizer, history=[], query=f\"USER: Is this low squamous or high squamous? ASSISTANT: {assistant_text1} USER: {user_text1} ASSISTANT: {assistant_text2} USER: {user_text2} ASSISTANT:\",  \n",
    "#                                             images=[image], \n",
    "#                                             template_version='chat')  \n",
    "\n",
    "inputs = {\n",
    "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
    "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
    "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
    "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
    "}\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    outputs = tokenizer.decode(outputs[0])\n",
    "    outputs = outputs.split(\"</s>\")[0]\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.load('mino_chat_exp6.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e8089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!deepspeed inference.py --deepspeed --deepspeed_config ds_inference_config.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e162859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d141691",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#41_jpg.rf.7c258d236d88a4304878a19405b61b5c.jpg\n",
    "\n",
    "#Glioma_T1_glioma (1).jpeg\n",
    "\n",
    "#dataset_frame064_stack_fov008.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the chances of bladder cancer recurrence within 1 months for a patient who had 1 tumors of placebo cm size, treated with 7 after undergoing transurethral surgical excision?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
